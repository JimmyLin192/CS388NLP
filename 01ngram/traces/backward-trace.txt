➜  01ngram git:(master) ✗ java codebases.BackwardBigramModel datasets/atis 0.1
# Train Sentences = 519 (# words = 3922)
# Test Sentences = 58 (# words = 431)
Training...
Perplexity = 7.973086336997878
Word Perplexity = 9.11755489082095
Testing...
Perplexity = 16.87696828562112
Word Perplexity = 21.713347433583955

➜  01ngram git:(master) ✗ java codebases.BackwardBigramModel datasets/wsj 0.1
# Train Sentences = 43820 (# words = 995626)
# Test Sentences = 4869 (# words = 111718)
Training...
Perplexity = 65.96170255020402
Word Perplexity = 71.04501288588204
Testing...
Perplexity = 185.02788191616528
Word Perplexity = 205.7519364350442

➜  01ngram git:(master) ✗ java codebases.BackwardBigramModel datasets/brown 0.1
# Train Sentences = 47207 (# words = 1079440)
# Test Sentences = 5245 (# words = 93530)
Training...
Perplexity = 82.37652060977345
Word Perplexity = 89.30691036127725
Testing...
Perplexity = 187.18673556551013
Word Perplexity = 218.7959152299797
